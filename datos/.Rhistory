}
#función para obtener la pendiente y los puntos de corte
calcularPendientePuntoCorte = function(pesos){
la_recta = c(-pesos[1]/pesos[2], -pesos[3]/pesos[2])
}
simetrias = apply(grises, 1, fsimetria)
intensidades = apply(grises, 1, mean)
par(mfrow=c(1,1))
datos = cbind(intensidades, simetrias)
etiquetas = digitos
#Ahora obtenemos el conjunto TEST:
digit.test <- read.table("datos/zip.test",
quote="\"", comment.char="", stringsAsFactors=FALSE)
digitos15.test = digit.test[digit.test$V1==4 | digit.test$V1==8,]
digitos_test = digitos15.test[,1]  # etiquetas
ndigitos = nrow(digitos15.test)
# se retira la clase y se monta una matriz 3D: 599*16*16
grises_test = array(unlist(subset(digitos15.test,select=-V1)),c(ndigitos,16,16))
rm(digit.test)
rm(digitos15.test)
#ahora calculamos regresión lineal para el conjunto de datos test:
simetrias_test = apply(grises_test, 1, fsimetria)
intensidades_test = apply(grises_test, 1, mean)
datos_test = cbind(intensidades_test, simetrias_test)
etiquetas_test = digitos_test
#--------funcion para calcular el error:
calcula_error = function(datos, pesos, etiquetas){
datos = cbind(datos,1)
num_mal_clasificados = 0
for(j in 1:nrow(datos)){
#realizamos el producto escalar para cada fila de la matriz de datos con su
# vector de pesos, actualiezando el vector de pesos en cada iteración:
escalar = crossprod(pesos, datos[j,])
if(sign(escalar) != sign(etiquetas[j])){
num_mal_clasificados = num_mal_clasificados+1
}
}
porcentaje = num_mal_clasificados/nrow(datos)
}
ajusta_PLA = function(datos,label,max_iter,vini){
#Añadimos una columna de unos a la matriz de datos:
datos = cbind(datos, 1)
#realizamos tantas iteraciones como max_iter tengamos
mal_clasificados = TRUE
i = 0
while((i < max_iter)& mal_clasificados){
mal_clasificados = FALSE
iterador = sample(nrow(datos)) #introducimos una compoente aletoria
for(j in iterador){
#realizamos el producto escalar para cada fila de la matriz de datos con su
# vector de pesos, actualiezando el vector de pesos en cada iteración:
escalar = crossprod(vini, datos[j,])
#dada la aleatoriadad implicita, en cuanto que encontremos uno salimos del bucle:
if(sign(escalar) != sign(label[j]) ){
vini = vini + label[j] *datos[j,]
mal_clasificados = TRUE
break
}
}
i=i+1
}
#calculamos la pendiente de la recta y el punto de corte
la_recta = c(-vini[1]/vini[2], -vini[3]/vini[2])
lista = list(pesos = vini, recta = la_recta, iteraciones = i)
lista
}
Regress_Lin = function(datos, label){
#x = intensidad, y = simetria
#cambiar etiquetas de 5 a 1 y de 1 a -1
#creamos una matriz con las simetrias:
X = cbind(datos, 1)
#creamos las etiquetas, cambiando 5 por unos:
Y = label
Y[Y==5]=-1
#obtenemos Singular Value Decomposition (SVD) de X por su traspuesta
SVD = svd(t(X)%*%X )
#CALCULEMOS AHORA LA PSEUDOINVERSA DE D:
PSEUDO_D = diag(1/SVD$d)
#Ahora para obtener la inversa de X por su traspuesta:
PSEUDO_X = SVD$v%*%PSEUDO_D%*%t(SVD$v)
#para obtener su pseudo inversa:
PSEUDO_X = PSEUDO_X%*%t(X)
#por último obtenemos el vector de pesos:
pesos = PSEUDO_X %*% Y
pesos
}
etiquetas[etiquetas==4]=-1
etiquetas[etiquetas==8]=1
ajusta_PLA_POCKET = function(datos,label,max_iter=200,vini = c(1,1,1)){
#e_in inicial
e_in_min = calcula_error(datos, vini, label)
vini_min = vini
perceptron_min = ajusta_PLA(datos, label, 1, vini_min)
print(vini)
for(i in 1:max_iter){
perceptron = ajusta_PLA(datos, label, 1, vini)
vini = perceptron$pesos
#calculamos el error en la iteracion que nos encontramos:
e_in = calcula_error(datos, vini, label)
if(e_in< e_in_min){
print("Mejorando")
e_in_min = e_in
vini_min = vini
perceptron_min = perceptron
}
}
#calculamos la pendiente de la recta y el punto de corte
la_recta = perceptron_min$recta
lista = list(pesos = vini_min, recta = la_recta, iteraciones = i-1)
lista
}
pesos_regresion = Regress_Lin(datos, etiquetas)
print(pesos_regresion)
perceptron_pocket = ajusta_PLA_POCKET(datos, etiquetas, 200, vini = pesos_regresion)
plot (datos, col=etiquetas+3)
abline(perceptron_pocket$recta[2], perceptron_pocket$recta[1])
etiquetas_test[etiquetas_test==4]=-1
etiquetas_test[etiquetas_test==8]=1
plot (datos_test, col=etiquetas_test+3)
abline(perceptron_pocket$recta[2], perceptron_pocket$recta[1])
e_in = calcula_error(datos, perceptron_pocket$pesos, etiquetas)
e_out = calcula_error(datos_test, perceptron_pocket$pesos, etiquetas_test)
print("El e_in es:")
e_in
print("El e_out es:")
e_out
e_in
#calculamos la cota
cota_in = e_in*100+ sqrt((8/length(etiquetas)) * log( (4*(2*length(etiquetas))^(3) +1)/0.05  ) )
cota_out = e_out*100+ sqrt((8/length(etiquetas_test)) * log( (4*(2*length(etiquetas_test))^(3) +1)/0.05  ) )
print("La cota del e_in:")
cota_in
print("La cota del e_out")
cota_out
simula_gaus = function(N=2,dim=2,mean = 0, sigma){
if (missing(sigma)) stop("Debe dar un vector de varianzas")
sigma = sqrt(sigma)  # para la generación se usa sd, y no la varianza
if(dim != length(sigma)) stop ("El numero de varianzas es distinto de la dimensión")
simula_gauss1 = function() rnorm(dim, sd = sigma, mean = mean) # genera 1 muestra, con las desviaciones especificadas
m = t(replicate(N,simula_gauss1())) # repite N veces, simula_gauss1 y se hace la traspuesta
m
}
datos = simula_gaus(113, 3, 0, c(1,1,1))
pesos = simula_gaus(1, 4, 0, c(1,1,1,1))
pesos[4] = 1 + pesos[4]
#Ahora generamos una funcion para obtener las etiquetas:
etiquetas_gaus = function(datos, pesos){
datos = cbind(datos, 1)
ruido = simula_gaus(1, 1, 0, 1)
f1 = function(x_n){
e = crossprod( t(pesos), x_n) + ruido*0.5
e
}
etiquetas = apply(datos, 1, f1)
}
etiquetas = etiquetas_gaus(datos, pesos)
RealizaParticiones = function(indices){
#hacemos un sample de los datos y de las etiquetas:
indices = sample(indices)
f2 = function(ini, fin){
test = indices[ini:fin]
train = indices[-(ini:fin)]
l = list(train = train, test = test)
l
}
p1=f2(1,13)
p2=f2(14,23)
p3=f2(24,33)
p4=f2(34,43)
p5=f2(44,53)
p6=f2(54,63)
p7=f2(64,73)
p8=f2(74,83)
p9=f2(84,93)
p10=f2(94,103)
p11=f2(104,113)
lista=list(p1=p1, p2=p2, p3=p3, p4=p4, p5=p5, p6=p6, p7=p7, p8=p8, p9=p9, p10=p10, p11=p11)
}
particiones = RealizaParticiones(1:length(etiquetas))
datos = cbind(datos, 1)
weightDecay = function(datos, indices, etiquetas, lambda){
#x = intensidad, y = simetria
#cambiar etiquetas de 5 a 1 y de 1 a -1
#creamos una matriz con las simetrias:
X = datos[indices,]
#creamos las etiquetas, cambiando 5 por unos:
Y = etiquetas[indices]
Y[Y==5]=-1
#obtenemos Singular Value Decomposition (SVD) de X por su traspuesta
SVD = svd(t(X)%*%X + lambda*diag(ncol = ncol(t(X)%*%X), nrow = nrow(t(X)%*%X)))
#CALCULEMOS AHORA LA PSEUDOINVERSA DE D:
PSEUDO_D = diag(1/SVD$d)
#Ahora para obtener la inversa de X por su traspuesta:
PSEUDO_X = SVD$v%*%PSEUDO_D%*%t(SVD$v)
#para obtener su pseudo inversa:
PSEUDO_X = PSEUDO_X%*%t(X)
#por último obtenemos el vector de pesos:
pesos = PSEUDO_X %*% Y
pesos
}
error = function(pesos, dato, etiqueta){
(pesos%*%dato -etiqueta)*(pesos%*%dato -etiqueta)
}
errores = numeric()
e_1 = numeric()
e_2 = numeric()
E_cv_total = numeric()
for(j in 1:1000){
#hacemos un sample de los datos.
particiones = RealizaParticiones(1:length(etiquetas))
for(i in 1:11){
peso_i = weightDecay(datos, particiones[[i]]$train, etiquetas, lambda=0.05/length(particiones[[i]]$train))
#ahora calculamos los errores:
e_i = sapply(particiones[[i]]$test, FUN=function(j){error(peso_i, datos[j,], etiquetas[j])} )
e_i = mean(e_i)
errores = c(errores, sum(e_i))
if(i == 3){
e_1 = c(e_1, errores[1])
e_2 = c(e_1, errores[2])
}
}
E_cv = mean(errores)
errores = numeric()
E_cv_total = c(E_cv_total, E_cv)
}
print("-----De e_1:")
print("media")
mean(e_1)
print("varianza")
var(e_1)
print("----De e_2:")
print("media")
mean(e_2)
print("varianza")
var(e_2)
print("-----De E_cv:")
print("media")
mean(E_cv_total)
print("varianza")
var(E_cv_total)
#declaramos la función E:
funcion_E = function(u,v){
e = exp(1)
r = (u^2*e^v-2*v^2*exp(-u))^2
r
}
#declaramos la funcion gradiente de E:
gradiente = function(u,v){
e = exp(1) #definimos e
#2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)+u^2*e^v-4*v*e^(-u))
g = c(2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)), 2*(u^2*e^v-2*v^2*e^(-u))*(u^2*e^v-4*v*e^(-u)))
g
}
#ahora la función gradiente descendente:
coordenadaDescendente = function(tasa_aprendizaje, funcion, el_gradiente){
#declaramos los pesos inicializados a 1
pesos = c(1,1)
#calculamos el gradiente:
anterior = funcion(pesos[1], pesos[2])
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones=1
while(abs(anterior - funcion(pesos[1], pesos[2]))>10^(-4)){
anterior = funcion(pesos[1], pesos[2])
pesos[1] = pesos[1] + tasa_aprendizaje*v_t[1]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
pesos[2]= pesos[2] + tasa_aprendizaje*v_t[2]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones = iteraciones+1
}
pesos_iteraciones = list(pesos = pesos, iteraciones = iteraciones)
}
pesos_iteraciones = coordenadaDescendente(0.1,funcion_E, gradiente)
print("Numero de iteraciones: ")
pesos_iteraciones$iteraciones
print("Los pesos obtenidos son: ")
pesos_iteraciones$pesos
#declaramos la función E:
funcion_E = function(u,v){
e = exp(1)
r = (u^2*e^v-2*v^2*exp(-u))^2
r
}
#declaramos la funcion gradiente de E:
gradiente = function(u,v){
e = exp(1) #definimos e
#2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)+u^2*e^v-4*v*e^(-u))
g = c(2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)), 2*(u^2*e^v-2*v^2*e^(-u))*(u^2*e^v-4*v*e^(-u)))
g
}
#ahora la función gradiente descendente:
coordenadaDescendente = function(tasa_aprendizaje, funcion, el_gradiente){
#declaramos los pesos inicializados a 1
pesos = c(1,1)
#calculamos el gradiente:
anterior = funcion(pesos[1], pesos[2])
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones=1
while(abs(anterior - funcion(pesos[1], pesos[2]))>10^(-4)){
anterior = funcion(pesos[1], pesos[2])
pesos[1] = pesos[1] + tasa_aprendizaje*v_t[1]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
pesos[2]= pesos[2] + tasa_aprendizaje*v_t[2]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones = iteraciones+1
}
pesos_iteraciones = list(pesos = pesos, iteraciones = iteraciones)
}
pesos_iteraciones = coordenadaDescendente(0.01,funcion_E, gradiente)
print("Numero de iteraciones: ")
pesos_iteraciones$iteraciones
print("Los pesos obtenidos son: ")
pesos_iteraciones$pesos
#declaramos la función E:
funcion_E = function(u,v){
e = exp(1)
r = (u^2*e^v-2*v^2*exp(-u))^2
r
}
#declaramos la funcion gradiente de E:
gradiente = function(u,v){
e = exp(1) #definimos e
#2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)+u^2*e^v-4*v*e^(-u))
g = c(2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)), 2*(u^2*e^v-2*v^2*e^(-u))*(u^2*e^v-4*v*e^(-u)))
g
}
#ahora la función gradiente descendente:
coordenadaDescendente = function(tasa_aprendizaje, funcion, el_gradiente){
#declaramos los pesos inicializados a 1
pesos = c(1,1)
#calculamos el gradiente:
anterior = funcion(pesos[1], pesos[2])
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones=1
while(abs(anterior - funcion(pesos[1], pesos[2]))>10^(-4)){
anterior = funcion(pesos[1], pesos[2])
pesos[1] = pesos[1] + tasa_aprendizaje*v_t[1]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
pesos[2]= pesos[2] + tasa_aprendizaje*v_t[2]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones = iteraciones+1
}
print(pesos)
pesos_iteraciones = list(pesos = pesos, iteraciones = iteraciones)
}
pesos_iteraciones = coordenadaDescendente(0.01,funcion_E, gradiente)
print("Numero de iteraciones: ")
pesos_iteraciones$iteraciones
print("Los pesos obtenidos son: ")
pesos_iteraciones$pesos
#declaramos la función E:
funcion_E = function(u,v){
e = exp(1)
r = (u^2*e^v-2*v^2*exp(-u))^2
r
}
#declaramos la funcion gradiente de E:
gradiente = function(u,v){
e = exp(1) #definimos e
#2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)+u^2*e^v-4*v*e^(-u))
g = c(2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)), 2*(u^2*e^v-2*v^2*e^(-u))*(u^2*e^v-4*v*e^(-u)))
g
}
#ahora la función gradiente descendente:
coordenadaDescendente = function(tasa_aprendizaje, funcion, el_gradiente){
#declaramos los pesos inicializados a 1
pesos = c(1,1)
#calculamos el gradiente:
anterior = funcion(pesos[1], pesos[2])
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones=1
while(abs(anterior - funcion(pesos[1], pesos[2]))>10^(-4) || iteraciones==1){
anterior = funcion(pesos[1], pesos[2])
pesos[1] = pesos[1] + tasa_aprendizaje*v_t[1]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
pesos[2]= pesos[2] + tasa_aprendizaje*v_t[2]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones = iteraciones+1
}
print(pesos)
pesos_iteraciones = list(pesos = pesos, iteraciones = iteraciones)
}
pesos_iteraciones = coordenadaDescendente(0.01,funcion_E, gradiente)
print("Numero de iteraciones: ")
pesos_iteraciones$iteraciones
print("Los pesos obtenidos son: ")
pesos_iteraciones$pesos
#declaramos la función E:
funcion_E = function(u,v){
e = exp(1)
r = (u^2*e^v-2*v^2*exp(-u))^2
r
}
#declaramos la funcion gradiente de E:
gradiente = function(u,v){
e = exp(1) #definimos e
#2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)+u^2*e^v-4*v*e^(-u))
g = c(2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)), 2*(u^2*e^v-2*v^2*e^(-u))*(u^2*e^v-4*v*e^(-u)))
g
}
#ahora la función gradiente descendente:
coordenadaDescendente = function(tasa_aprendizaje, funcion, el_gradiente){
#declaramos los pesos inicializados a 1
pesos = c(1,1)
#calculamos el gradiente:
anterior = funcion(pesos[1], pesos[2])
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones=1
while(abs(anterior - funcion(pesos[1], pesos[2]))>10^(-4) || iteraciones==1){
anterior = funcion(pesos[1], pesos[2])
#Actualizamos primero la coordenada 'u':
pesos[1] = pesos[1] + tasa_aprendizaje*v_t[1]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
#Ahora actualizamos la coordenada 'v', empleando los pesos anteriores
pesos[2]= pesos[2] + tasa_aprendizaje*v_t[2]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones = iteraciones+1
}
print(pesos)
pesos_iteraciones = list(pesos = pesos, iteraciones = iteraciones)
}
pesos_iteraciones = coordenadaDescendente(0.1,funcion_E, gradiente)
print("Numero de iteraciones: ")
pesos_iteraciones$iteraciones
print("Los pesos obtenidos son: ")
pesos_iteraciones$pesos
#declaramos la función E:
funcion_E = function(u,v){
e = exp(1)
r = (u^2*e^v-2*v^2*exp(-u))^2
r
}
#declaramos la funcion gradiente de E:
gradiente = function(u,v){
e = exp(1) #definimos e
#2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)+u^2*e^v-4*v*e^(-u))
g = c(2*(u^2*e^v-2*v^2*e^(-u))*(2*e^v*u+2*v^2*e^(-u)), 2*(u^2*e^v-2*v^2*e^(-u))*(u^2*e^v-4*v*e^(-u)))
g
}
#ahora la función gradiente descendente:
coordenadaDescendente = function(tasa_aprendizaje, funcion, el_gradiente){
#declaramos los pesos inicializados a 1
pesos = c(1,1)
#calculamos el gradiente:
anterior = funcion(pesos[1], pesos[2])
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones=1
while(abs(anterior - funcion(pesos[1], pesos[2]))>10^(-4) || iteraciones==1){
print(funcion(pesos[1], pesos[2]))
anterior = funcion(pesos[1], pesos[2])
#Actualizamos primero la coordenada 'u':
pesos[1] = pesos[1] + tasa_aprendizaje*v_t[1]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
#Ahora actualizamos la coordenada 'v', empleando los pesos anteriores
pesos[2]= pesos[2] + tasa_aprendizaje*v_t[2]
g = el_gradiente(pesos[1],pesos[2])
v_t = -g
iteraciones = iteraciones+1
}
print(pesos)
pesos_iteraciones = list(pesos = pesos, iteraciones = iteraciones)
}
pesos_iteraciones = coordenadaDescendente(0.1,funcion_E, gradiente)
print("Numero de iteraciones: ")
pesos_iteraciones$iteraciones
print("Los pesos obtenidos son: ")
pesos_iteraciones$pesos
knitr::opts_chunk$set(echo = TRUE)
setwd("~")
auto
library("ISLR", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
auto
Auto
colnames(Auto)
sumary(Auto)
summary(Auto)
anyNA(Auto)
dim (Auto)
lmm
lm
?lm
atributes(Auto)
methods(class(Auto))
library(readr)
cardio <- read_csv("~/cardio.csv")
View(cardio)
View(cardio)
cardio[1,]
cardio[1,]
cardio[2,]
cardio[3,]
cardio[1,]
cardio[1,]
cardio[2,]
cardio[2,]
clear
